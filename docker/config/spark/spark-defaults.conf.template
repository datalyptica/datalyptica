# Spark Configuration Template
# This file is processed at runtime to substitute environment variables

# Application Configuration
spark.app.name                   ShuDL-Spark
spark.master                     ${SPARK_MASTER_URL}
spark.submit.deployMode          client

# Memory Configuration
spark.driver.memory              1g
spark.driver.maxResultSize       512m
spark.executor.memory            1g
spark.executor.cores             1

# SQL Configuration
spark.sql.adaptive.enabled       true
spark.sql.adaptive.coalescePartitions.enabled true

# Iceberg Catalog Configuration (REST)
spark.sql.catalog.nessie                    org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.nessie.catalog-impl      org.apache.iceberg.rest.RESTCatalog
spark.sql.catalog.nessie.uri               ${NESSIE_URI}
spark.sql.catalog.nessie.warehouse         ${WAREHOUSE_LOCATION}
spark.sql.catalog.nessie.s3.endpoint       ${S3_ENDPOINT}
spark.sql.catalog.nessie.s3.path-style-access true

# Legacy Iceberg Configuration (for compatibility)
spark.sql.catalog.iceberg.type   nessie
spark.sql.catalog.iceberg.uri    ${NESSIE_URI}
spark.sql.catalog.iceberg.ref    main
spark.sql.catalog.iceberg.authentication.type NONE
spark.sql.catalog.iceberg.warehouse ${WAREHOUSE_LOCATION}

# S3 Configuration (credentials provided via environment variables)
spark.hadoop.fs.s3a.endpoint    ${S3_ENDPOINT}
spark.hadoop.fs.s3a.access.key  ${S3_ACCESS_KEY}
spark.hadoop.fs.s3a.secret.key  ${S3_SECRET_KEY}
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.impl        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Spark UI Configuration
spark.ui.port                    4040
spark.ui.retainedJobs            100
spark.ui.retainedStages          100
spark.ui.retainedTasks           100

# Spark History Server Configuration
spark.history.ui.port            18080
spark.history.fs.logDirectory    ${WAREHOUSE_LOCATION}/spark-history
spark.history.fs.update.interval 10s
spark.history.retainedApplications 50

# Spark SQL Configuration
spark.sql.warehouse.dir          ${WAREHOUSE_LOCATION}/spark-warehouse
spark.sql.catalogImplementation hive

# Delta Lake Configuration (if using Delta)
spark.sql.extensions            io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog  org.apache.spark.sql.delta.catalog.DeltaCatalog

# Serialization Configuration
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator           org.apache.iceberg.spark.IcebergKryoRegistrator

# Network Configuration
spark.sql.adaptive.advisoryPartitionSizeInBytes 128MB
spark.sql.adaptive.skewJoin.enabled true

# Performance Configuration
spark.sql.adaptive.localShuffleReader.enabled true
spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled true
