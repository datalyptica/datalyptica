# Docker Secrets Configuration
secrets:
  # Core Infrastructure
  postgres_password:
    file: ../../secrets/passwords/postgres_password
  postgres_replication_password:
    file: ../../secrets/passwords/postgres_replication_password

  # Platform Services
  datalyptica_password:
    file: ../../secrets/passwords/datalyptica_password
  nessie_password:
    file: ../../secrets/passwords/nessie_password

  # Identity & Access
  keycloak_admin_password:
    file: ../../secrets/passwords/keycloak_admin_password
  keycloak_db_password:
    file: ../../secrets/passwords/keycloak_db_password

  # Storage
  minio_root_password:
    file: ../../secrets/passwords/minio_root_password
  s3_access_key:
    file: ../../secrets/passwords/s3_access_key
  s3_secret_key:
    file: ../../secrets/passwords/s3_secret_key

  # Analytics & ML
  airflow_password:
    file: ../../secrets/passwords/airflow_password
  jupyterhub_password:
    file: ../../secrets/passwords/jupyterhub_password
  mlflow_password:
    file: ../../secrets/passwords/mlflow_password
  superset_password:
    file: ../../secrets/passwords/superset_password

  # Monitoring
  grafana_admin_password:
    file: ../../secrets/passwords/grafana_admin_password
  clickhouse_password:
    file: ../../secrets/passwords/clickhouse_password

services:
  # MinIO Object Storage
  minio:
    image: ghcr.io/datalyptica/datalyptica/minio:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-minio
    ports:
      - "${MINIO_API_PORT}:9000" # API
      - "${MINIO_CONSOLE_PORT}:9001" # Console
      - "${MINIO_API_HTTPS_PORT:-9443}:9443" # HTTPS API
    env_file:
      - .env.minio
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_VOLUMES=/data
      - MINIO_OPTS=--console-address :9001
      - MINIO_REGION=${MINIO_REGION}
      - MINIO_DEFAULT_BUCKETS=${MINIO_BUCKET_NAME}

      # Additional MinIO Configuration
      - MINIO_ADDRESS=${MINIO_ADDRESS}
      - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
      - MINIO_VOLUMES=${MINIO_VOLUMES}
      - MINIO_LOG_LEVEL=${MINIO_LOG_LEVEL}
      - MINIO_LOG_FILE=${MINIO_LOG_FILE}
      - MINIO_CACHE_DRIVES=${MINIO_CACHE_DRIVES}
      - MINIO_CACHE_EXPIRY=${MINIO_CACHE_EXPIRY}
      - MINIO_CACHE_QUOTA=${MINIO_CACHE_QUOTA}
      - MINIO_CACHE_EXCLUDE=${MINIO_CACHE_EXCLUDE}
      - MINIO_COMPRESS=${MINIO_COMPRESS}
      - MINIO_COMPRESS_MIME_TYPES=${MINIO_COMPRESS_MIME_TYPES}
      - MINIO_BROWSER=${MINIO_BROWSER}

      # SSL/TLS Configuration
      - MINIO_CERTS_DIR=/certs
      - MINIO_SERVER_URL=${MINIO_SERVER_URL:-https://localhost:9443}
    volumes:
      - minio_data:/data
      - ../secrets/certificates/minio:/certs:ro
      - ../secrets/certificates/ca:/ca:ro
    secrets:
      - minio_root_password
      - s3_access_key
      - s3_secret_key
    networks:
      - storage_network
      - data_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  # ==================================================================================
  # STORAGE LAYER - PostgreSQL (Standalone Dev Mode)
  # ==================================================================================

  postgresql:
    image: ghcr.io/datalyptica/datalyptica/postgresql:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-postgresql
    hostname: postgresql
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    environment:
      # PostgreSQL Superuser
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres

      # PostgreSQL Application User
      - DATALYPTICA_USER=${POSTGRES_USER:-datalyptica}
      - DATALYPTICA_PASSWORD_FILE=/run/secrets/datalyptica_password
      - DATALYPTICA_DB=${DATALYPTICA_DB:-datalyptica}

      # Nessie Database
      - NESSIE_DB=${NESSIE_DB:-nessie}

      # Keycloak Database
      - KEYCLOAK_DB=${KEYCLOAK_DB:-keycloak}
      - KEYCLOAK_DB_USER=${KEYCLOAK_DB_USER:-keycloak}
      - KEYCLOAK_DB_PASSWORD_FILE=/run/secrets/keycloak_db_password

      # PostgreSQL Data Directory
      - PGDATA=/var/lib/postgresql/data/pgdata

      # PostgreSQL Configuration
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgresql_data:/var/lib/postgresql/data
    secrets:
      - postgres_password
      - datalyptica_password
      - keycloak_db_password
    networks:
      - storage_network
      - data_network
      - control_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 4G
        reservations:
          cpus: "2.0"
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U postgres || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # Nessie Catalog Service
  nessie:
    image: ghcr.io/datalyptica/datalyptica/nessie:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-nessie
    ports:
      - "${NESSIE_PORT}:19120"
      - "${NESSIE_HTTPS_PORT:-19443}:19443"
    env_file:
      - .env.nessie
    environment:
      # Server Configuration
      - QUARKUS_HTTP_PORT=${NESSIE_PORT}
      - QUARKUS_HTTP_HOST=${NESSIE_HOST}
      - QUARKUS_HTTP_SSL_PORT=${NESSIE_HTTPS_PORT:-19443}
      - QUARKUS_HTTP_SSL_CERTIFICATE_FILE=/etc/nessie/certs/server-cert.pem
      - QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILE=/etc/nessie/certs/server-key.pem
      - QUARKUS_HTTP_INSECURE_REQUESTS=enabled

      # CORS Configuration
      - QUARKUS_HTTP_CORS=${NESSIE_CORS_ENABLED}
      - QUARKUS_HTTP_CORS_ORIGINS=${NESSIE_CORS_ORIGINS}
      - QUARKUS_HTTP_CORS_METHODS=${NESSIE_CORS_METHODS}
      - QUARKUS_HTTP_CORS_HEADERS=${NESSIE_CORS_HEADERS}
      - QUARKUS_HTTP_CORS_EXPOSED_HEADERS=${NESSIE_CORS_EXPOSED_HEADERS}
      - QUARKUS_HTTP_CORS_ACCESS_CONTROL_MAX_AGE=${NESSIE_CORS_MAX_AGE}

      # Database Configuration - Use default datasource (no name)
      - NESSIE_VERSION_STORE_TYPE=${NESSIE_VERSION_STORE_TYPE}

      # Default Quarkus datasource configuration
      - QUARKUS_DATASOURCE_DB_KIND=postgresql
      - QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/nessie
      - QUARKUS_DATASOURCE_USERNAME=datalyptica
      - QUARKUS_DATASOURCE_PASSWORD=fHx7kgvF9iTQVbOzEpnahelNBMYmqQIQ
      - QUARKUS_DATASOURCE_JDBC_INITIAL_SIZE=${NESSIE_DB_INITIAL_SIZE}
      - QUARKUS_DATASOURCE_JDBC_MIN_SIZE=${NESSIE_DB_MIN_SIZE}
      - QUARKUS_DATASOURCE_JDBC_MAX_SIZE=${NESSIE_DB_MAX_SIZE}

      # PostgreSQL connection for entrypoint health check
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=nessie
      - POSTGRES_USER=datalyptica
      - POSTGRES_PASSWORD=fHx7kgvF9iTQVbOzEpnahelNBMYmqQIQ

      # Catalog Configuration
      - NESSIE_CATALOG_DEFAULT_WAREHOUSE=${NESSIE_DEFAULT_WAREHOUSE}
      - NESSIE_CATALOG_WAREHOUSES_WAREHOUSE_LOCATION=${NESSIE_WAREHOUSE_LOCATION}
      - WAREHOUSE_LOCATION=${WAREHOUSE_LOCATION}

      # Metrics Configuration
      - NESSIE_MICROMETER_ENABLED=${NESSIE_MICROMETER_ENABLED}
      - NESSIE_MICROMETER_EXPORT_PROMETHEUS_ENABLED=${NESSIE_MICROMETER_EXPORT_PROMETHEUS_ENABLED}

      # Health Check Configuration
      - NESSIE_HEALTH_ROOT_PATH=${NESSIE_HEALTH_ROOT_PATH}

      # OpenAPI Configuration
      - NESSIE_SWAGGER_UI_ALWAYS_INCLUDE=${NESSIE_SWAGGER_UI_ALWAYS_INCLUDE}
      - NESSIE_SWAGGER_UI_PATH=${NESSIE_SWAGGER_UI_PATH}

      # Authentication Configuration
      - NESSIE_SERVER_AUTHENTICATION_ENABLED=${NESSIE_SERVER_AUTHENTICATION_ENABLED}

      # S3/MinIO Configuration
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - MINIO_REGION=${MINIO_REGION}
    volumes:
      - ../secrets/certificates/nessie:/etc/nessie/certs:ro
      - ../secrets/certificates/ca:/etc/nessie/ca:ro
    networks:
      - storage_network
      - data_network
    depends_on:
      - postgresql
      - minio
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v2/config"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: 5
      start_period: 60s

  # Trino Query Engine
  trino:
    image: ghcr.io/datalyptica/datalyptica/trino:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-trino
    ports:
      - "${TRINO_PORT}:8080"
    environment:
      # Server Configuration
      - TRINO_COORDINATOR=${TRINO_COORDINATOR}
      - TRINO_NODE_SCHEDULER_INCLUDE_COORDINATOR=${TRINO_INCLUDE_COORDINATOR}
      - TRINO_HTTP_SERVER_PORT=${TRINO_PORT}
      - TRINO_HTTPS_PORT=${TRINO_HTTPS_PORT:-8443}
      - TRINO_HTTPS_ENABLED=${TRINO_HTTPS_ENABLED:-true}
      - TRINO_HTTPS_KEYSTORE_PATH=${TRINO_HTTPS_KEYSTORE_PATH:-/etc/trino/certs/server-cert.pem}
      - TRINO_HTTPS_KEYSTORE_KEY=${TRINO_HTTPS_KEYSTORE_KEY:-/etc/trino/certs/server-key.pem}
      - TRINO_DISCOVERY_URI=${TRINO_DISCOVERY_URI}
      - TRINO_QUERY_MAX_MEMORY=${TRINO_QUERY_MAX_MEMORY}
      - TRINO_MEMORY_HEAP_HEADROOM_PER_NODE=${TRINO_MEMORY_HEAP_HEADROOM}

      # Node Configuration
      - TRINO_NODE_ENVIRONMENT=${TRINO_NODE_ENVIRONMENT}
      - TRINO_NODE_DATA_DIR=${TRINO_NODE_DATA_DIR}
      - TRINO_NODE_ID=${TRINO_NODE_ID}

      # JVM Configuration
      - TRINO_JVM_XMX=${TRINO_JVM_XMX}
      - TRINO_JVM_GC=${TRINO_JVM_GC}
      - TRINO_JVM_G1_HEAP_REGION_SIZE=${TRINO_JVM_G1_HEAP_REGION_SIZE}
      - TRINO_JVM_EXIT_ON_OOM=${TRINO_JVM_EXIT_ON_OOM}
      - TRINO_JVM_USE_GC_OVERHEAD_LIMIT=${TRINO_JVM_USE_GC_OVERHEAD_LIMIT}
      - TRINO_JVM_NIO_MAX_CACHED_BUFFER_SIZE=${TRINO_JVM_NIO_MAX_CACHED_BUFFER_SIZE}
      - TRINO_JVM_ALLOW_ATTACH_SELF=${TRINO_JVM_ALLOW_ATTACH_SELF}

      # Logging Configuration
      - TRINO_LOG_LEVEL_ROOT=${TRINO_LOG_LEVEL_ROOT}
      - TRINO_LOG_LEVEL_SERVER=${TRINO_LOG_LEVEL_SERVER}
      - TRINO_LOG_LEVEL_EXECUTION=${TRINO_LOG_LEVEL_EXECUTION}
      - TRINO_LOG_LEVEL_METADATA=${TRINO_LOG_LEVEL_METADATA}
      - TRINO_LOG_LEVEL_SECURITY=${TRINO_LOG_LEVEL_SECURITY}
      - TRINO_LOG_LEVEL_SPI=${TRINO_LOG_LEVEL_SPI}
      - TRINO_LOG_LEVEL_SQL=${TRINO_LOG_LEVEL_SQL}
      - TRINO_LOG_LEVEL_TRANSACTION=${TRINO_LOG_LEVEL_TRANSACTION}
      - TRINO_LOG_LEVEL_TYPE=${TRINO_LOG_LEVEL_TYPE}
      - TRINO_LOG_LEVEL_UTIL=${TRINO_LOG_LEVEL_UTIL}
      - TRINO_LOG_LEVEL_HTTP_REQUEST=${TRINO_LOG_LEVEL_HTTP_REQUEST}

      # Iceberg Catalog Configuration (Nessie-based)
      - TRINO_CATALOG_ICEBERG_CONNECTOR_NAME=icebergoka
      - TRINO_CATALOG_ICEBERG_CATALOG_TYPE=${TRINO_CATALOG_ICEBERG_CATALOG_TYPE}
      - TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_URI=${TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_URI}
      - TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_REF=${TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_REF}
      - TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_DEFAULT_WAREHOUSE_DIR=${TRINO_CATALOG_ICEBERG_NESSIE_CATALOG_DEFAULT_WAREHOUSE_DIR}
      - TRINO_CATALOG_ICEBERG_FILE_FORMAT=${TRINO_CATALOG_ICEBERG_FILE_FORMAT}
      - TRINO_CATALOG_ICEBERG_COMPRESSION_CODEC=${TRINO_CATALOG_ICEBERG_COMPRESSION_CODEC}

      # S3/MinIO Configuration
      - TRINO_CATALOG_ICEBERG_S3_ENDPOINT=${S3_ENDPOINT}
      - TRINO_CATALOG_ICEBERG_S3_REGION=${S3_REGION}
      - TRINO_CATALOG_ICEBERG_S3_PATH_STYLE_ACCESS=${S3_PATH_STYLE_ACCESS}
      - TRINO_CATALOG_ICEBERG_S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - TRINO_CATALOG_ICEBERG_S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_REGION=${S3_REGION}
      - S3_PATH_STYLE_ACCESS=${S3_PATH_STYLE_ACCESS}
      - FS_NATIVE_S3_ENABLED=true
    volumes:
      - trino_data:/data
      - ../../configs/trino:/usr/lib/trino/etc:ro
      - ../secrets/certificates/trino:/certs/trino:ro
      - ../secrets/certificates/ca:/certs/ca:ro
    networks:
      - data_network
      - storage_network
    depends_on:
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 8G
        reservations:
          cpus: "2.0"
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: 60s

  # Spark Master
  spark-master:
    image: ghcr.io/datalyptica/datalyptica/spark:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-spark-master
    ports:
      - "${SPARK_UI_PORT}:4040" # Spark UI
      - "${SPARK_MASTER_PORT}:7077" # Spark Master
    environment:
      # Spark Core Configuration
      - SPARK_MODE=master
      - SPARK_WEBUI_PORT=${SPARK_UI_PORT}
      - SPARK_RPC_AUTHENTICATION_ENABLED=false
      - SPARK_RPC_ENCRYPTION_ENABLED=true
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=true
      - SPARK_SSL_ENABLED=true
      - SPARK_SSL_KEYSTORE=/opt/spark/certs/server-cert.pem
      - SPARK_SSL_TRUSTSTORE=/opt/spark/ca/ca-cert.pem
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
      - SPARK_DRIVER_MEMORY=${SPARK_DRIVER_MEMORY}
      - SPARK_DRIVER_MAX_RESULT_SIZE=${SPARK_DRIVER_MAX_RESULT_SIZE}
      - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY}
      - SPARK_EXECUTOR_CORES=${SPARK_EXECUTOR_CORES}
      - SPARK_EXECUTOR_INSTANCES=${SPARK_EXECUTOR_INSTANCES}
      - SPARK_DYNAMIC_ALLOCATION_ENABLED=${SPARK_DYNAMIC_ALLOCATION_ENABLED}
      - SPARK_SERIALIZER=${SPARK_SERIALIZER}

      # Original working Spark configuration (no AWS region conflicts)
      - SPARK_MODE=master
      - SPARK_WEBUI_PORT=4040
      - SPARK_RPC_AUTHENTICATION_ENABLED=false
      - SPARK_RPC_ENCRYPTION_ENABLED=true
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=true
      - SPARK_SSL_ENABLED=true
      - SPARK_SSL_KEYSTORE=/opt/spark/certs/server-cert.pem
      - SPARK_SSL_TRUSTSTORE=/opt/spark/ca/ca-cert.pem
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
      # Environment variables for template processing (original approach)
      - NESSIE_URI=${NESSIE_URI}
      - WAREHOUSE_LOCATION=${WAREHOUSE_LOCATION}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_REGION=${S3_REGION}
      - S3_PATH_STYLE_ACCESS=${S3_PATH_STYLE_ACCESS}
      - MINIO_REGION=${MINIO_REGION}
      # Template processing variables
      - SPARK_ICEBERG_CATALOG_NAME=${SPARK_ICEBERG_CATALOG_NAME}
      - SPARK_ICEBERG_REF=${SPARK_ICEBERG_REF}
      - SPARK_ICEBERG_URI=${SPARK_ICEBERG_URI}
      - SPARK_ICEBERG_WAREHOUSE=${SPARK_ICEBERG_WAREHOUSE}
      - SPARK_ICEBERG_IO_IMPL=${SPARK_ICEBERG_IO_IMPL}
      # AWS Configuration for Iceberg S3FileIO
      - AWS_REGION=${S3_REGION}
      - AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}
      # Spark Configuration Directory
      - SPARK_CONF_DIR=/tmp/spark/conf
    volumes:
      - ../secrets/certificates/spark:/opt/spark/certs:ro
      - ../secrets/certificates/ca:/opt/spark/ca:ro
    networks:
      - data_network
      - storage_network
    depends_on:
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 4G
        reservations:
          cpus: "2.0"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4040"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: 60s

  # Spark Worker
  spark-worker:
    image: ghcr.io/datalyptica/datalyptica/spark:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-spark-worker
    ports:
      - "4041:4040" # Spark Worker UI
    environment:
      # Spark Core Configuration
      - SPARK_MODE=worker
      - SPARK_RPC_AUTHENTICATION_ENABLED=false
      - SPARK_RPC_ENCRYPTION_ENABLED=true
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=true
      - SPARK_SSL_ENABLED=true
      - SPARK_SSL_KEYSTORE=/opt/spark/certs/server-cert.pem
      - SPARK_SSL_TRUSTSTORE=/opt/spark/ca/ca-cert.pem
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY}
      - SPARK_EXECUTOR_CORES=${SPARK_EXECUTOR_CORES}

      # Spark Iceberg Configuration (for Nessie integration)
      - SPARK_ICEBERG_CATALOG_NAME=${SPARK_ICEBERG_CATALOG_NAME}
      - SPARK_ICEBERG_URI=${SPARK_ICEBERG_URI}
      - SPARK_ICEBERG_REF=${SPARK_ICEBERG_REF}
      - SPARK_ICEBERG_WAREHOUSE=${SPARK_ICEBERG_WAREHOUSE}
      - SPARK_ICEBERG_IO_IMPL=${SPARK_ICEBERG_IO_IMPL}

      # Original working Spark worker configuration (template-based approach)
      - SPARK_MODE=worker
      - SPARK_RPC_AUTHENTICATION_ENABLED=false
      - SPARK_RPC_ENCRYPTION_ENABLED=true
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=true
      - SPARK_SSL_ENABLED=true
      - SPARK_SSL_KEYSTORE=/opt/spark/certs/server-cert.pem
      - SPARK_SSL_TRUSTSTORE=/opt/spark/ca/ca-cert.pem
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      # Environment variables for template processing
      - NESSIE_URI=${NESSIE_URI}
      - WAREHOUSE_LOCATION=${WAREHOUSE_LOCATION}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_REGION=${S3_REGION}
      - S3_PATH_STYLE_ACCESS=${S3_PATH_STYLE_ACCESS}
      - MINIO_REGION=${MINIO_REGION}
      # Template processing variables
      - SPARK_ICEBERG_CATALOG_NAME=${SPARK_ICEBERG_CATALOG_NAME}
      - SPARK_ICEBERG_REF=${SPARK_ICEBERG_REF}
      - SPARK_ICEBERG_URI=${SPARK_ICEBERG_URI}
      - SPARK_ICEBERG_WAREHOUSE=${SPARK_ICEBERG_WAREHOUSE}
      - SPARK_ICEBERG_IO_IMPL=${SPARK_ICEBERG_IO_IMPL}
      # AWS Configuration for Iceberg S3FileIO
      - AWS_REGION=${S3_REGION}
      - AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}
      # Spark Configuration Directory
      - SPARK_CONF_DIR=/tmp/spark/conf
    volumes:
      - ../secrets/certificates/spark:/opt/spark/certs:ro
      - ../secrets/certificates/ca:/opt/spark/ca:ro
    networks:
      - data_network
      - storage_network
      - control_network
    depends_on:
      spark-master:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 4G
        reservations:
          cpus: "2.0"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4040"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: 60s

  # Apache Kafka (KRaft Mode - No ZooKeeper Required)
  kafka:
    image: ghcr.io/datalyptica/datalyptica/kafka:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-kafka
    ports:
      - "${KAFKA_PORT:-9092}:9092"
      - "9093:9093"
      - "9094:9094" # Controller port for KRaft
    environment:
      # KRaft Mode Configuration
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_NODE_ID=1
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka:9094
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - CLUSTER_ID=xR9VObP1SvO9P4tYW3VApw

      # Listeners Configuration (PLAINTEXT only for dev mode)
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT

      # Replication & Topic Configuration
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_LOG_RETENTION_HOURS=168
      - KAFKA_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_NUM_PARTITIONS=3

      # Storage Configuration
      - KAFKA_LOG_DIRS=/var/lib/kafka/data
    volumes:
      - kafka_data:/var/lib/kafka/data
      - ../secrets/certificates/kafka:/etc/kafka/certs:ro
      - ../secrets/certificates/ca:/etc/kafka/ca:ro
    networks:
      - control_network
      - data_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test:
        [
          "CMD",
          "kafka-broker-api-versions",
          "--bootstrap-server",
          "localhost:9092",
        ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Confluent Schema Registry
  schema-registry:
    image: ghcr.io/datalyptica/datalyptica/schema-registry:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-schema-registry
    ports:
      - "${SCHEMA_REGISTRY_PORT:-8085}:8081"
      - "${SCHEMA_REGISTRY_HTTPS_PORT:-8445}:8443"
    environment:
      - SCHEMA_REGISTRY_HOST_NAME=schema-registry
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=kafka:9092
      - SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081
      - SCHEMA_REGISTRY_KAFKASTORE_TOPIC=_schemas
      - SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR=1
      - SCHEMA_REGISTRY_DEBUG=false
      - SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL=BACKWARD
      - SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL=INFO
    volumes:
      - schema_registry_data:/var/lib/schema-registry
    networks:
      - control_network
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 256M
    healthcheck:
      test: ["CMD", "/usr/local/bin/healthcheck.sh"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Kafka UI (Management Interface)
  kafka-ui:
    image: ghcr.io/datalyptica/datalyptica/kafka-ui:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-kafka-ui
    ports:
      - "${KAFKA_UI_PORT:-8090}:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=datalyptica-cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
      - KAFKA_CLUSTERS_0_METRICS_PORT=9997
      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://schema-registry:8081
    networks:
      - management_network
      - control_network
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:8080/actuator/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Flink JobManager
  flink-jobmanager:
    image: ghcr.io/datalyptica/datalyptica/flink:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-flink-jobmanager
    ports:
      - "${FLINK_JOBMANAGER_PORT:-8081}:8081"
      - "6123:6123"
    command: jobmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      NESSIE_URI: ${NESSIE_URI:-http://nessie:19120/api/v2}
      WAREHOUSE_LOCATION: ${WAREHOUSE_LOCATION:-s3a://lakehouse/warehouse}
      S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_PATH_STYLE_ACCESS: "true"
      FLINK_SSL_ENABLED: "true"
      FLINK_SSL_KEYSTORE: "/opt/flink/certs/server-cert.pem"
      FLINK_SSL_TRUSTSTORE: "/opt/flink/ca/ca-cert.pem"
    volumes:
      - flink_checkpoints:/opt/flink/checkpoints
      - flink_savepoints:/opt/flink/savepoints
      - ../secrets/certificates/flink:/opt/flink/certs:ro
      - ../secrets/certificates/ca:/opt/flink/ca:ro
    networks:
      - data_network
      - control_network
    depends_on:
      kafka:
        condition: service_healthy
      nessie:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Flink TaskManager
  flink-taskmanager:
    image: ghcr.io/datalyptica/datalyptica/flink:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-flink-taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    command: taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      TASK_MANAGER_NUMBER_OF_TASK_SLOTS: ${FLINK_TASK_SLOTS:-4}
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      NESSIE_URI: ${NESSIE_URI:-http://nessie:19120/api/v2}
      WAREHOUSE_LOCATION: ${WAREHOUSE_LOCATION:-s3a://lakehouse/warehouse}
      S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_PATH_STYLE_ACCESS: "true"
      FLINK_SSL_ENABLED: "true"
      FLINK_SSL_KEYSTORE: "/opt/flink/certs/server-cert.pem"
      FLINK_SSL_TRUSTSTORE: "/opt/flink/ca/ca-cert.pem"
    volumes:
      - flink_checkpoints:/opt/flink/checkpoints
      - flink_savepoints:/opt/flink/savepoints
      - ../secrets/certificates/flink:/opt/flink/certs:ro
      - ../secrets/certificates/ca:/opt/flink/ca:ro
    networks:
      - data_network
      - control_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://flink-jobmanager:8081/taskmanagers"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Kafka Connect with Debezium (Avro Support)
  kafka-connect:
    image: ghcr.io/datalyptica/datalyptica/kafka-connect:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-kafka-connect
    ports:
      - "${KAFKA_CONNECT_PORT:-8083}:8083"
    environment:
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
      - CONNECT_REST_PORT=8083
      - CONNECT_REST_ADVERTISED_PORT=8083
      - CONNECT_LISTENERS=http://0.0.0.0:8083
      - CONNECT_GROUP_ID=debezium-connect-cluster
      - CONNECT_CONFIG_STORAGE_TOPIC=debezium-connect-configs
      - CONNECT_OFFSET_STORAGE_TOPIC=debezium-connect-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=debezium-connect-status
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
      # Avro Converters with Schema Registry
      - CONNECT_KEY_CONVERTER=io.confluent.connect.avro.AvroConverter
      - CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - CONNECT_VALUE_CONVERTER=io.confluent.connect.avro.AvroConverter
      - CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect
      - CONNECT_PLUGIN_PATH=/usr/share/confluent-hub-components
    volumes:
      - kafka_connect_data:/var/lib/kafka-connect
    networks:
      - data_network
      - control_network
      - storage_network
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      postgresql:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # ClickHouse (OLAP database)
  clickhouse:
    image: ghcr.io/datalyptica/datalyptica/clickhouse:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-clickhouse
    ports:
      - "${CLICKHOUSE_HTTP_PORT:-8123}:8123"
      - "${CLICKHOUSE_NATIVE_PORT:-9000}:9000"
    environment:
      - CLICKHOUSE_DB=${CLICKHOUSE_DB:-default}
      - CLICKHOUSE_USER=${CLICKHOUSE_USER:-default}
      - CLICKHOUSE_PASSWORD_FILE=/run/secrets/clickhouse_password
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
      - CLICKHOUSE_HTTPS_PORT=${CLICKHOUSE_HTTPS_PORT:-8443}
      - CLICKHOUSE_TCP_PORT_SECURE=${CLICKHOUSE_NATIVE_SECURE_PORT:-9440}
      - CLICKHOUSE_OPENSSL_SERVER_CERT_FILE=/etc/clickhouse-server/certs/server-cert.pem
      - CLICKHOUSE_OPENSSL_SERVER_KEY_FILE=/etc/clickhouse-server/certs/server-key.pem
      - CLICKHOUSE_OPENSSL_CA_CERT_FILE=/etc/clickhouse-server/ca/ca-cert.pem
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      - ../secrets/certificates/clickhouse:/etc/clickhouse-server/certs:ro
      - ../secrets/certificates/ca:/etc/clickhouse-server/ca:ro
    secrets:
      - clickhouse_password
    networks:
      - storage_network
      - data_network
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 4G
        reservations:
          cpus: "2.0"
          memory: 1G
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # dbt for SQL Transformations
  dbt:
    image: ghcr.io/datalyptica/datalyptica/dbt:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-dbt
    ports:
      - "${DBT_DOCS_PORT:-8580}:8580"
    environment:
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - SPARK_HOST=spark-master
      - SPARK_PORT=10000
      - DBT_PROFILES_DIR=/root/.dbt
    volumes:
      - dbt_project:/dbt
      - dbt_profiles:/root/.dbt
    networks:
      - data_network
      - storage_network
    depends_on:
      trino:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 256M
    command: ["sh", "-c", "sleep infinity"]

  # ==================================================================================
  # DATA QUALITY LAYER
  # ==================================================================================

  # Great Expectations - Data Quality & Validation
  great-expectations:
    image: ghcr.io/datalyptica/datalyptica/great-expectations:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-great-expectations
    hostname: great-expectations
    ports:
      - "${GE_JUPYTER_PORT:-8888}:8888" # JupyterLab
    env_file:
      - .env.great-expectations
    environment:
      # Great Expectations Configuration
      - GE_HOME=${GE_HOME:-/opt/great_expectations}
      - GE_JUPYTER_TOKEN=${GE_JUPYTER_TOKEN:-datalyptica-ge-token}
      - JUPYTER_ENABLE_LAB=yes

      # Database Connections
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=${DATALYPTICA_DB:-datalyptica}
      - POSTGRES_USER=${DATALYPTICA_USER:-datalyptica}
      - POSTGRES_PASSWORD=${DATALYPTICA_PASSWORD:-datalyptica123}

      # Trino Connection
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - TRINO_CATALOG=iceberg
      - TRINO_SCHEMA=default

      # MinIO/S3 Connection
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_REGION=${S3_REGION}
      - S3_BUCKET=${MINIO_BUCKET_NAME:-lakehouse}

      # Nessie Connection
      - NESSIE_URI=${NESSIE_URI}
      - NESSIE_REF=main
      - NESSIE_WAREHOUSE=s3://lakehouse/warehouse

      # Spark Configuration
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}

    volumes:
      - ge_data:/opt/great_expectations
      - ge_notebooks:/opt/notebooks
      - ../../configs/great-expectations:/opt/great_expectations/config:ro
    networks:
      - data_network
      - storage_network
      - management_network
    depends_on:
      postgresql:
        condition: service_healthy
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
      trino:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "1.0"
          memory: 2G
    healthcheck:
      test:
        ["CMD", "python", "-c", "import great_expectations; print('healthy')"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: 60s

  # ==================================================================================
  # OBSERVABILITY LAYER
  # ==================================================================================

  # Prometheus Monitoring
  prometheus:
    image: ghcr.io/datalyptica/datalyptica/prometheus:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    environment:
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - ../../configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../configs/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - ../secrets/certificates/prometheus:/etc/prometheus/certs:ro
      - ../secrets/certificates/ca:/etc/prometheus/ca:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME:-15d}"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
    networks:
      - management_network
      - data_network
      - storage_network
      - control_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 512M
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}
      start_period: ${HEALTHCHECK_START_PERIOD:-60s}

  # Grafana Dashboards
  grafana:
    image: ghcr.io/datalyptica/datalyptica/grafana:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_admin_password
      - GF_USERS_ALLOW_SIGN_UP=${GRAFANA_ALLOW_SIGN_UP:-false}
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
      - GF_SERVER_PROTOCOL=${GRAFANA_PROTOCOL:-http}
      - GF_SERVER_HTTP_PORT=${GRAFANA_PORT:-3000}
      - GF_INSTALL_PLUGINS=${GRAFANA_INSTALL_PLUGINS:-}
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - grafana_data:/var/lib/grafana
      - ../../configs/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../../configs/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ../secrets/certificates/grafana:/etc/grafana/certs:ro
      - ../secrets/certificates/ca:/etc/grafana/ca:ro
    secrets:
      - grafana_admin_password
    networks:
      - management_network
    depends_on:
      prometheus:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 256M
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "--no-check-certificate",
          "https://localhost:3443/api/health",
        ]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}
      start_period: ${HEALTHCHECK_START_PERIOD:-60s}

  # Loki Log Aggregation
  loki:
    image: ghcr.io/datalyptica/datalyptica/loki:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-loki
    ports:
      - "${LOKI_PORT:-3100}:3100"
      - "${LOKI_HTTPS_PORT:-3143}:3143"
    environment:
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - ../../configs/loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - ../secrets/certificates/loki:/etc/loki/certs:ro
      - ../secrets/certificates/ca:/etc/loki/ca:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - management_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3100/ready",
        ]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}
      start_period: ${HEALTHCHECK_START_PERIOD:-60s}

  # Grafana Alloy Log Collector (Promtail successor)
  alloy:
    image: ghcr.io/datalyptica/datalyptica/alloy:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-alloy
    volumes:
      - ../../configs/loki/alloy-config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - alloy_data:/var/lib/alloy
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy
    ports:
      - "${ALLOY_HTTP_PORT:-12345}:12345" # Alloy HTTP API
    networks:
      - management_network
      - data_network
      - storage_network
      - control_network
    depends_on:
      loki:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'alloy.*run' || exit 1"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}
      start_period: ${HEALTHCHECK_START_PERIOD:-30s}

  # Alertmanager for Alert Routing and Notifications
  alertmanager:
    image: ghcr.io/datalyptica/datalyptica/alertmanager:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-alertmanager
    ports:
      - "${ALERTMANAGER_PORT:-9095}:9093"
    environment:
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - ../../configs/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ../../configs/alertmanager/templates:/etc/alertmanager/templates:ro
      - ../secrets/certificates/alertmanager:/etc/alertmanager/certs:ro
      - ../secrets/certificates/ca:/etc/alertmanager/ca:ro
      - alertmanager_data:/alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.listen-address=:9093"
    networks:
      - management_network
    depends_on:
      prometheus:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9093/-/healthy",
        ]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}
      start_period: ${HEALTHCHECK_START_PERIOD:-30s}

  # Keycloak Identity and Access Management
  keycloak:
    image: ghcr.io/datalyptica/datalyptica/keycloak:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-keycloak
    ports:
      - "${KEYCLOAK_PORT:-8180}:8080"
      - "${KEYCLOAK_HTTPS_PORT:-8543}:8443"
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        export KC_DB_PASSWORD=$$(cat /run/secrets/keycloak_db_password)
        export KEYCLOAK_ADMIN_PASSWORD=$$(cat /run/secrets/keycloak_admin_password)
        /opt/keycloak/bin/kc.sh start-dev
    environment:
      # Database Configuration
      - KC_DB=postgres
      - KC_DB_URL=jdbc:postgresql://${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${KEYCLOAK_DB:-keycloak}
      - KC_DB_USERNAME=${KEYCLOAK_DB_USER:-keycloak}

      # Admin User
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN_USER:-admin}

      # Keycloak Configuration
      - KC_HOSTNAME=${KEYCLOAK_HOSTNAME:-localhost}
      - KC_HOSTNAME_PORT=${KEYCLOAK_HTTPS_PORT:-8543}
      - KC_HOSTNAME_STRICT=false
      - KC_HOSTNAME_STRICT_HTTPS=false
      - KC_HTTP_ENABLED=true
      - KC_HTTPS_ENABLED=true
      - KC_HTTPS_CERTIFICATE_FILE=/opt/keycloak/certs/server-cert.pem
      - KC_HTTPS_CERTIFICATE_KEY_FILE=/opt/keycloak/certs/server-key.pem
      - KC_HEALTH_ENABLED=true
      - KC_METRICS_ENABLED=true

      # Proxy Configuration
      - KC_PROXY=edge
      - KC_HOSTNAME_STRICT_BACKCHANNEL=false
    volumes:
      - keycloak_data:/opt/keycloak/data
      - ../secrets/certificates/keycloak:/opt/keycloak/certs:ro
      - ../secrets/certificates/ca:/opt/keycloak/ca:ro
    secrets:
      - keycloak_admin_password
      - keycloak_db_password
    networks:
      - management_network
      - storage_network
      - data_network
    depends_on:
      - postgresql
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "exec 3<>/dev/tcp/localhost/8080 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200\\|UP' || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ==================================================================================
  # Analytics & ML Services
  # ==================================================================================

  # Redis Cache
  redis:
    image: ghcr.io/datalyptica/datalyptica/redis:${DATALYPTICA_VERSION:-v1.0.0}
    container_name: ${COMPOSE_PROJECT_NAME}-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - data_network
      - management_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.25"
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # JupyterHub - Multi-user Jupyter Notebook Environment
  jupyterhub:
    build:
      context: ../docker/jupyterhub
      dockerfile: Dockerfile
    image: ghcr.io/datalyptica/datalyptica/jupyterhub:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-jupyterhub
    ports:
      - "${JUPYTERHUB_PORT:-8000}:8000"
      - "${JUPYTERHUB_HUB_PORT:-8082}:8081"
    environment:
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - JUPYTERHUB_DB_NAME=${JUPYTERHUB_DB_NAME}
      - JUPYTERHUB_DB_USER=${JUPYTERHUB_DB_USER}
      - JUPYTERHUB_DB_PASSWORD=${JUPYTERHUB_DB_PASSWORD}
      - JUPYTERHUB_ADMIN_USER=${JUPYTERHUB_ADMIN_USER}
      - JUPYTERHUB_OPEN_SIGNUP=${JUPYTERHUB_OPEN_SIGNUP}
      - JUPYTERHUB_IDLE_TIMEOUT=${JUPYTERHUB_IDLE_TIMEOUT}
      - JUPYTERHUB_LOG_LEVEL=${JUPYTERHUB_LOG_LEVEL}
      - JUPYTERHUB_DEBUG=${JUPYTERHUB_DEBUG}
      - JUPYTERHUB_PROXY_AUTH_TOKEN=${JUPYTERHUB_PROXY_AUTH_TOKEN}
      - DATALYPTICA_DB=${DATALYPTICA_DB}
      - DATALYPTICA_USER=${DATALYPTICA_USER}
      - DATALYPTICA_PASSWORD=${DATALYPTICA_PASSWORD}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - NESSIE_URI=${NESSIE_URI}
      - COMPOSE_PROJECT_NAME=${COMPOSE_PROJECT_NAME}
      - DATALYPTICA_VERSION=${DATALYPTICA_VERSION}
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - jupyterhub_data:/srv/jupyterhub/data
      - ../../configs/jupyterhub:/srv/jupyterhub/config:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - data_network
      - storage_network
      - management_network
    depends_on:
      postgresql:
        condition: service_healthy
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "0.5"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/hub/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # MLflow - ML Experiment Tracking and Model Registry
  mlflow:
    build:
      context: ../docker/mlflow
      dockerfile: Dockerfile
    image: ghcr.io/datalyptica/datalyptica/mlflow:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-mlflow
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    environment:
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_USER=${POSTGRES_USER}
      - MLFLOW_DB_NAME=${MLFLOW_DB_NAME}
      - MLFLOW_DB_USER=${MLFLOW_DB_USER}
      - MLFLOW_DB_PASSWORD=${MLFLOW_DB_PASSWORD}
      - MLFLOW_BUCKET=${MLFLOW_BUCKET}
      - MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}
      - AWS_REGION=${S3_REGION}
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - mlflow_data:/opt/mlflow/data
    networks:
      - data_network
      - storage_network
      - management_network
    depends_on:
      postgresql:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "0.5"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Apache Superset - Modern BI Platform
  superset:
    build:
      context: ../docker/superset
      dockerfile: Dockerfile
    image: ghcr.io/datalyptica/datalyptica/superset:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-superset
    ports:
      - "${SUPERSET_PORT:-8088}:8088"
    environment:
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - SUPERSET_DB_NAME=${SUPERSET_DB_NAME}
      - SUPERSET_DB_USER=${SUPERSET_DB_USER}
      - SUPERSET_DB_PASSWORD=${SUPERSET_DB_PASSWORD}
      - SUPERSET_ADMIN_USERNAME=${SUPERSET_ADMIN_USERNAME}
      - SUPERSET_ADMIN_EMAIL=${SUPERSET_ADMIN_EMAIL}
      - SUPERSET_ADMIN_PASSWORD=${SUPERSET_ADMIN_PASSWORD}
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
      - SUPERSET_USER_REGISTRATION=${SUPERSET_USER_REGISTRATION}
      - SUPERSET_LOG_LEVEL=${SUPERSET_LOG_LEVEL}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_DB=${REDIS_DB}
      - MAPBOX_API_KEY=${MAPBOX_API_KEY}
      - TIMEZONE=${TIMEZONE:-UTC}
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - superset_data:/app/superset_home
      - ../../configs/superset:/app/superset_home/config:ro
    networks:
      - data_network
      - storage_network
      - management_network
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
      trino:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 8G
        reservations:
          cpus: "1.0"
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Apache Airflow - Workflow Orchestration
  airflow-webserver:
    build:
      context: ../docker/airflow
      dockerfile: Dockerfile
    image: ghcr.io/datalyptica/datalyptica/airflow:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-airflow-webserver
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8091}:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CELERY__BROKER_URL=redis://${REDIS_HOST}:${REDIS_PORT}/${AIRFLOW_REDIS_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - POSTGRES_HOST=${POSTGRES_HOST:-postgresql}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_USER=${POSTGRES_USER}
      - AIRFLOW_DB_NAME=${AIRFLOW_DB_NAME}
      - AIRFLOW_DB_USER=${AIRFLOW_DB_USER}
      - AIRFLOW_DB_PASSWORD=${AIRFLOW_DB_PASSWORD}
      - AIRFLOW_ADMIN_USERNAME=${AIRFLOW_ADMIN_USERNAME}
      - AIRFLOW_ADMIN_EMAIL=${AIRFLOW_ADMIN_EMAIL}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - AIRFLOW_REDIS_DB=${AIRFLOW_REDIS_DB}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../../configs/airflow:/opt/airflow/config:ro
    networks:
      - data_network
      - storage_network
      - management_network
      - control_network
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "0.5"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  airflow-scheduler:
    image: ghcr.io/datalyptica/datalyptica/airflow:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-airflow-scheduler
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CELERY__BROKER_URL=redis://${REDIS_HOST}:${REDIS_PORT}/${AIRFLOW_REDIS_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../../configs/airflow:/opt/airflow/config:ro
    networks:
      - data_network
      - storage_network
      - control_network
    depends_on:
      - airflow-webserver
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "0.5"
          memory: 1G

  airflow-worker:
    image: ghcr.io/datalyptica/datalyptica/airflow:${DATALYPTICA_VERSION}
    container_name: ${COMPOSE_PROJECT_NAME}-airflow-worker
    command: worker
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CELERY__BROKER_URL=redis://${REDIS_HOST}:${REDIS_PORT}/${AIRFLOW_REDIS_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST:-postgresql}:${POSTGRES_PORT:-5432}/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - TZ=${TIMEZONE:-UTC}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../../configs/airflow:/opt/airflow/config:ro
    networks:
      - data_network
      - storage_network
      - control_network
    depends_on:
      - airflow-webserver
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 8G
        reservations:
          cpus: "1.0"
          memory: 2G

volumes:
  minio_data:
    driver: local
  postgresql_data:
    driver: local
  trino_data:
    driver: local
  kafka_data:
    driver: local
  schema_registry_data:
    driver: local
  kafka_connect_data:
    driver: local
  flink_checkpoints:
    driver: local
  flink_savepoints:
    driver: local
  clickhouse_data:
    driver: local
  clickhouse_logs:
    driver: local
  dbt_project:
    driver: local
  dbt_profiles:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
  alloy_data:
    driver: local
  alertmanager_data:
    driver: local
  keycloak_data:
    driver: local
  ge_data:
    driver: local
  ge_notebooks:
    driver: local

  # Analytics & ML Services Volumes
  redis_data:
    driver: local
  jupyterhub_data:
    driver: local
  mlflow_data:
    driver: local
  superset_data:
    driver: local
  airflow_dags:
    driver: local
  airflow_logs:
    driver: local
  airflow_plugins:
    driver: local

networks:
  # Management Network - Monitoring and observability services
  management_network:
    name: ${COMPOSE_PROJECT_NAME}_management
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

  # Control Network - Orchestration and coordination services
  control_network:
    name: ${COMPOSE_PROJECT_NAME}_control
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

  # Data Network - Processing engines and compute services
  data_network:
    name: ${COMPOSE_PROJECT_NAME}_data
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/16

  # Storage Network - Data persistence and storage services
  storage_network:
    name: ${COMPOSE_PROJECT_NAME}_storage
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/16
