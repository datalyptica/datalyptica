# Airflow Configuration for Datalyptica Platform

[core]
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
executor = CeleryExecutor
sql_alchemy_conn = postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${AIRFLOW_DB_NAME}
load_examples = False
max_active_runs_per_dag = 3
parallelism = 32
dag_concurrency = 16
max_active_tasks_per_dag = 16
default_timezone = ${TIMEZONE:-UTC}

[database]
sql_alchemy_pool_size = 20
sql_alchemy_pool_recycle = 3600
sql_alchemy_pool_pre_ping = True

[webserver]
base_url = http://localhost:${AIRFLOW_WEBSERVER_PORT}
web_server_port = ${AIRFLOW_WEBSERVER_PORT}
expose_config = True
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth
secret_key = ${AIRFLOW_SECRET_KEY}
workers = 4
worker_class = sync

[celery]
broker_url = redis://${REDIS_HOST}:${REDIS_PORT}/${AIRFLOW_REDIS_DB:-0}
result_backend = db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${AIRFLOW_DB_NAME}
worker_concurrency = 16
worker_prefetch_multiplier = 4

[celery_broker_transport_options]
visibility_timeout = 21600

[scheduler]
scheduler_heartbeat_sec = 5
min_file_process_interval = 30
dag_dir_list_interval = 60
max_threads = 2
catchup_by_default = False

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
logging_level = INFO
fab_logging_level = WARN
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[api]
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
enable_experimental_api = False

[operators]
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_queue = default
