# Datalyptica Comprehensive Test Suite - Execution Guide

## ğŸ¯ What Was Created

A complete testing framework for all **21 components** of the Datalyptica Data Lakehouse platform:

### Test Files Created

```
tests/
â”œâ”€â”€ run-tests.sh                              # ğŸš€ Main test runner
â”œâ”€â”€ test-summary.sh                           # ğŸ“Š Quick component status check
â”œâ”€â”€ comprehensive-test-all-21-components.sh   # ğŸ”¬ Complete test suite
â”œâ”€â”€ README.md                                 # ğŸ“– Detailed documentation
â”‚
â”œâ”€â”€ helpers/
â”‚   â””â”€â”€ test_helpers.sh                       # ğŸ› ï¸ Reusable test utilities
â”‚
â”œâ”€â”€ health/
â”‚   â””â”€â”€ test-all-health.sh                    # â¤ï¸ Health checks for 21 components
â”‚
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ test-data-flow.sh                     # ğŸ”„ Cross-component integration tests
â”‚
â””â”€â”€ e2e/
    â””â”€â”€ test-complete-pipeline.sh             # ğŸŒ End-to-end pipeline validation
```

## ğŸ§ª Test Coverage (11 Phases)

### Phase 1: Pre-Flight Checks

- âœ… Docker environment validation
- âœ… Docker Compose availability
- âœ… Project file verification
- âœ… Environment configuration

### Phase 2: Component Health Checks (21 Components)

All components across 5 layers:

1. **Storage** (3): MinIO, PostgreSQL, Nessie
2. **Streaming** (4): Zookeeper, Kafka, Schema Registry, Kafka UI
3. **Processing** (4): Spark Master/Worker, Flink JobManager/TaskManager
4. **Analytics** (4): Trino, ClickHouse, dbt, Kafka Connect
5. **Observability** (6): Prometheus, Grafana, Loki, Alloy, Alertmanager, Keycloak

### Phase 3: Network Connectivity

- HTTP endpoint availability tests
- Port accessibility verification
- API responsiveness checks

### Phase 4: Storage Layer Integration

- MinIO bucket operations
- PostgreSQL database queries
- Nessie catalog operations

### Phase 5: Streaming Layer Integration

- Kafka topic creation/deletion
- Message production/consumption
- Schema Registry operations

### Phase 6: Processing Layer Integration

- Spark cluster status
- Flink cluster operations
- Job submission capabilities

### Phase 7: Query Engine Integration

- Trino SQL query execution
- ClickHouse analytical queries
- Cross-engine data access

### Phase 8: Observability Stack

- Prometheus metrics scraping
- Grafana datasource connectivity
- Loki log ingestion

### Phase 9: Security & IAM

- Keycloak health and readiness
- Realm accessibility tests

### Phase 10: End-to-End Data Flow

- Complete pipeline testing
- Multi-component workflows
- Data consistency verification

### Phase 11: Component Interdependency

- Service-to-service communication
- Dependency chain validation

## ğŸš€ Quick Start

### 1. Start All Services

```bash
cd docker
docker compose up -d

# Wait for services to start (2-3 minutes)
watch -n 2 'docker compose ps'
```

### 2. Check Component Status

```bash
# Quick status check
./tests/test-summary.sh
```

**Expected Output:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          Datalyptica Component Test Summary                         â•‘
â•‘          Testing All 21 Platform Components                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â• Storage Layer (3 components) â•â•â•
âœ“ minio
âœ“ postgresql
âœ“ nessie

â•â•â• Streaming Layer (4 components) â•â•â•
âœ“ zookeeper
âœ“ kafka
âœ“ schema-registry
âœ“ kafka-ui

... (continues for all 21 components)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                  Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“ Healthy:   21 / 21
â—‹ Running:   0 / 21
âœ— Down:      0 / 21
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‰ All systems operational! (100%)
```

### 3. Run Health Checks

```bash
# Quick health check (1-2 minutes)
./tests/run-tests.sh quick
```

### 4. Run Comprehensive Tests

```bash
# Full test suite (5-10 minutes)
./tests/run-tests.sh full
```

## ğŸ“‹ Test Execution Options

### Option 1: Quick Health Check (Recommended for CI)

```bash
./tests/run-tests.sh quick
```

- â±ï¸ Duration: 1-2 minutes
- âœ… Tests: Component health checks only
- ğŸ¯ Use: Quick validation, CI pipelines

### Option 2: Health-Only Tests

```bash
./tests/run-tests.sh health
```

- â±ï¸ Duration: 1-2 minutes
- âœ… Tests: Detailed health checks for all 21 components

### Option 3: Integration Tests

```bash
./tests/run-tests.sh integration
```

- â±ï¸ Duration: 3-5 minutes
- âœ… Tests: Cross-component data flow
- ğŸ”„ Tests: Kafka â†’ Iceberg, Trino â†” Spark, ClickHouse

### Option 4: End-to-End Tests

```bash
./tests/run-tests.sh e2e
```

- â±ï¸ Duration: 5-7 minutes
- âœ… Tests: Complete pipeline simulation
- ğŸŒ Scenario: IoT sensor data pipeline

### Option 5: Comprehensive Test Suite

```bash
./tests/comprehensive-test-all-21-components.sh
```

- â±ï¸ Duration: 10-15 minutes
- âœ… Tests: All 11 phases
- ğŸ“Š Coverage: 100% of components

## ğŸ¯ Test Scenarios Covered

### Scenario 1: Storage Layer Validation

```
MinIO â†’ PostgreSQL â†’ Nessie â†’ Iceberg Tables
```

**Tests:**

- Bucket creation/deletion
- Database connectivity
- Catalog operations
- Table metadata storage

### Scenario 2: Streaming Pipeline

```
Data Source â†’ Kafka â†’ Flink â†’ Iceberg
```

**Tests:**

- Topic creation
- Message production/consumption
- Stream processing
- Data persistence

### Scenario 3: SQL Analytics

```
Iceberg â† Trino â†’ Query Results
```

**Tests:**

- Schema creation
- Table creation
- Data insertion
- Query execution
- Cross-engine access

### Scenario 4: Real-Time OLAP

```
Kafka â†’ ClickHouse â†’ Materialized Views â†’ Analytics
```

**Tests:**

- Kafka engine tables
- Real-time ingestion
- Aggregation queries
- Performance metrics

### Scenario 5: Complete IoT Pipeline (E2E)

```
IoT Sensors â†’ Kafka â†’ Flink â†’ Iceberg â†’ Trino â†’ ClickHouse â†’ Power BI
```

**Tests:**

- Data ingestion
- Stream processing
- Storage in Data Lake
- SQL analytics
- OLAP analytics
- Monitoring & observability

## ğŸ“Š Understanding Test Results

### Success Output

```
âœ… Component is healthy (24s)

========================================
        Test Summary
========================================
âœ… Passed: 45
âŒ Failed: 0
ğŸ“Š Total:  45
========================================
ğŸ‰ All tests passed!
```

### Partial Failure Output

```
âŒ Component health check failed

========================================
        Test Summary
========================================
âœ… Passed: 38
âŒ Failed: 7
ğŸ“Š Total:  45
========================================
ğŸ’¥ Some tests failed
```

## ğŸ› Troubleshooting Failed Tests

### Common Failures and Solutions

#### 1. Service Not Running

```
âŒ postgresql health check failed
```

**Solution:**

```bash
# Check service status
docker ps -a | grep datalyptica-postgresql

# Restart service
docker restart datalyptica-postgresql

# Check logs
docker logs datalyptica-postgresql
```

#### 2. Port Already in Use

```
Error: bind: address already in use
```

**Solution:**

```bash
# Find process using port
lsof -i :9092  # Example for Kafka port

# Kill process or change port in docker/.env
```

#### 3. Insufficient Resources

```
âŒ Container exited with code 137
```

**Solution:**

```bash
# Increase Docker resources
# Docker Desktop â†’ Settings â†’ Resources
# RAM: 8GB+ (recommended: 16GB)
# CPUs: 4+ (recommended: 8)
```

#### 4. Dependency Not Ready

```
âŒ Nessie health check failed
```

**Solution:**

```bash
# Check dependencies
docker logs datalyptica-nessie | grep -i error

# Restart with dependencies
docker compose up -d postgresql
sleep 10
docker compose up -d nessie
```

## ğŸ” Detailed Component Testing

### Test Individual Component

```bash
# Example: Test MinIO
curl http://localhost:9000/minio/health/live

# Example: Test Nessie
curl http://localhost:19120/api/v2/config

# Example: Test Trino
docker exec datalyptica-trino trino --execute "SHOW CATALOGS"

# Example: Test Kafka
docker exec datalyptica-kafka kafka-topics --bootstrap-server localhost:9092 --list

# Example: Test ClickHouse
docker exec datalyptica-clickhouse clickhouse-client --query "SELECT 1"
```

## ğŸ“ˆ CI/CD Integration

### GitHub Actions Workflow

```yaml
name: Datalyptica Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up environment
        run: cp docker/.env.example docker/.env

      - name: Start services
        run: |
          cd docker
          docker compose up -d

      - name: Wait for services
        run: sleep 120

      - name: Run health checks
        run: ./tests/run-tests.sh quick

      - name: Run integration tests
        run: ./tests/run-tests.sh integration

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: tests/*.log
```

## ğŸ“ Test Development Guide

### Adding New Component Test

1. **Add to health check** (`tests/health/test-all-health.sh`):

```bash
test_step "22. New Component (Description)"
if http_health_check "http://localhost:PORT/health"; then
    test_info "âœ… New Component is healthy"
else
    test_error "âŒ New Component health check failed"
fi
```

2. **Add integration test** (`tests/integration/test-data-flow.sh`):

```bash
test_step "Test: New Component integration"
# Add test logic here
```

3. **Update comprehensive test** (`comprehensive-test-all-21-components.sh`):

```bash
# Add to appropriate phase
test_step "Testing New Component..."
# Add test logic
```

4. **Update documentation**:

- Update component count in README.md
- Add component to architecture diagram
- Document new test scenarios

## ğŸ“ Best Practices

### 1. Test Isolation

- Each test creates its own resources
- Cleanup after test completion
- Use unique identifiers (e.g., `test-$$`)

### 2. Timeouts

- All tests use timeouts to prevent hanging
- Default: 30-120 seconds per operation
- Adjust based on resource availability

### 3. Error Handling

- Tests continue on non-critical failures
- Clear error messages
- Comprehensive logging

### 4. Idempotency

- Tests can be run multiple times
- No side effects between runs
- Proper cleanup on failure

## ğŸ“š Additional Resources

- **Test Helpers**: See `tests/helpers/test_helpers.sh` for reusable functions
- **Docker Compose**: See `docker/docker-compose.yml` for service definitions
- **Configuration**: See `docker/.env.example` for environment variables
- **Architecture**: See `.github/copilot-instructions.md` for platform overview

## ğŸ†˜ Getting Help

If tests fail:

1. âœ… Check service logs: `docker logs datalyptica-<component>`
2. âœ… Verify services running: `docker ps | grep datalyptica-`
3. âœ… Check resource usage: `docker stats`
4. âœ… Review test output for specific errors
5. âœ… Check GitHub Issues for known problems

## ğŸ‰ Success Criteria

All tests passing indicates:

- âœ… All 21 components are operational
- âœ… Network connectivity is working
- âœ… Storage layer is functional
- âœ… Streaming pipeline is operational
- âœ… Processing engines are running
- âœ… Query engines are accessible
- âœ… Observability stack is collecting data
- âœ… Security layer is protecting resources
- âœ… End-to-end data flow is working
- âœ… Platform is ready for production use

---

**Last Updated**: November 26, 2025
**Test Suite Version**: 1.0.0
**Components Tested**: 21/21 (100%)
