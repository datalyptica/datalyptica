# Analytics & ML Services Integration

**Integration Date**: December 1, 2024  
**Version**: v1.0.0  
**Status**: ğŸ¯ Ready for Build

---

## Overview

Four enterprise-grade analytics and ML services have been integrated into the Datalyptica platform:

1. **JupyterHub** - Multi-user notebook environment
2. **MLflow** - ML experiment tracking and model registry
3. **Apache Superset** - Modern BI and data visualization
4. **Apache Airflow** - Workflow orchestration and scheduling

These services complete the platform's data science and analytics capabilities, providing end-to-end support for ML workflows, data exploration, visualization, and pipeline orchestration.

---

## Services Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Analytics & ML Layer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  JupyterHub  â”‚  â”‚    MLflow    â”‚  â”‚   Superset   â”‚          â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ Notebooks &  â”‚  â”‚ Experiment   â”‚  â”‚ Dashboards & â”‚          â”‚
â”‚  â”‚ Development  â”‚  â”‚ Tracking     â”‚  â”‚ Visualizationâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                  Apache Airflow                          â”‚     â”‚
â”‚  â”‚  (Webserver + Scheduler + Worker)                       â”‚     â”‚
â”‚  â”‚  Pipeline Orchestration & Scheduling                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚    Redis     â”‚  (Cache & Message Broker)                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Data Platform Layer (Iceberg)                     â”‚
â”‚    Trino + Spark + PostgreSQL + MinIO + Nessie                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Service Details

### 1. JupyterHub (Port 8000)

**Purpose**: Multi-user Jupyter notebook environment for collaborative data science

**Features**:

- Multi-user authentication with NativeAuthenticator
- Docker-based notebook spawner (each user gets isolated container)
- Persistent user workspaces
- Pre-configured access to all platform services
- Idle culler to manage resources
- Admin dashboard for user management

**Key Integrations**:

- PostgreSQL: User database and session management
- MinIO: Access to S3 data
- Trino: Query Iceberg tables
- Nessie: Data versioning
- MLflow: Experiment tracking from notebooks

**Spawned Notebook Image**:

- Base: Jupyter SciPy Notebook (Python 3.11)
- Packages: PySpark, Trino, Great Expectations, MLflow, dbt, pandas, scikit-learn
- JDK 11 for Spark operations

**Access**:

- URL: http://localhost:8000
- Admin user: Configured via `JUPYTERHUB_ADMIN_USER`
- Authentication: Native (DB-based), self-registration optional

### 2. MLflow (Port 5000)

**Purpose**: ML experiment tracking, model registry, and artifact storage

**Features**:

- Experiment tracking with metrics, parameters, and artifacts
- Model registry with versioning and staging
- Model serving capabilities
- Integration with scikit-learn, PyTorch, TensorFlow, XGBoost
- S3-compatible artifact storage (MinIO)
- RESTful API for programmatic access

**Key Integrations**:

- PostgreSQL: Metadata store (experiments, runs, models)
- MinIO: Artifact storage (models, plots, datasets)
- JupyterHub: Track experiments from notebooks
- Airflow: MLOps pipelines
- Spark: Distributed training tracking

**Access**:

- URL: http://localhost:5000
- No authentication (internal network only)
- Python client: `mlflow.set_tracking_uri("http://mlflow:5000")`

### 3. Apache Superset (Port 8088)

**Purpose**: Modern BI platform for data exploration and visualization

**Features**:

- SQL Lab for interactive queries
- Rich visualization library (50+ chart types)
- Dashboard builder with drag-and-drop
- Native filters and cross-filtering
- Caching with Redis
- Async query execution with Celery
- Scheduled reports and alerts
- Role-based access control

**Key Integrations**:

- PostgreSQL: Metadata database
- Redis: Caching and async query results
- Trino: Query engine for Iceberg/Nessie
- ClickHouse: Real-time analytics
- Spark: SQL interface (via Thrift server)

**Pre-configured Connections**:

- Trino: `trino://trino@trino:8080`
- ClickHouse: `clickhouse://clickhouse:8123`
- PostgreSQL: `postgresql://haproxy:5000`

**Access**:

- URL: http://localhost:8088
- Admin credentials: Configured via env vars
- Authentication: Flask-AppBuilder (DB-based)

### 4. Apache Airflow (Port 8090)

**Purpose**: Workflow orchestration and pipeline scheduling

**Architecture**:

- **Webserver**: Web UI and REST API
- **Scheduler**: DAG parsing and task scheduling
- **Worker**: Task execution (Celery-based)

**Features**:

- DAG-based workflow definition (Python)
- Rich operator library (Postgres, S3, Spark, Trino, Docker)
- Task dependencies and conditional logic
- Retry and alerting mechanisms
- Extensive monitoring and logging
- Connection and variable management
- RBAC and authentication

**Key Integrations**:

- PostgreSQL: Metadata database
- Redis: Celery broker and result backend
- Trino: SQL operations via TrinoOperator
- Spark: SparkSubmitOperator for ETL jobs
- Docker: DockerOperator for containerized tasks
- MLflow: Model training pipelines
- Great Expectations: Data quality checks

**Pre-installed Providers**:

- `apache-airflow-providers-postgres`
- `apache-airflow-providers-amazon` (S3)
- `apache-airflow-providers-apache-spark`
- `apache-airflow-providers-trino`
- `apache-airflow-providers-redis`
- `apache-airflow-providers-docker`

**Access**:

- URL: http://localhost:8090
- Admin credentials: Configured via env vars
- Authentication: Basic auth

---

## Configuration

### Environment Variables

All configuration is in `docker/.env`:

```bash
# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# JupyterHub
JUPYTERHUB_PORT=8000
JUPYTERHUB_DB_NAME=jupyterhub
JUPYTERHUB_DB_USER=jupyterhub
JUPYTERHUB_DB_PASSWORD=jupyterhub123
JUPYTERHUB_ADMIN_USER=admin
JUPYTERHUB_OPEN_SIGNUP=False
JUPYTERHUB_IDLE_TIMEOUT=3600

# MLflow
MLFLOW_PORT=5000
MLFLOW_DB_NAME=mlflow
MLFLOW_DB_USER=mlflow
MLFLOW_DB_PASSWORD=mlflow123
MLFLOW_BUCKET=mlflow-artifacts

# Superset
SUPERSET_PORT=8088
SUPERSET_DB_NAME=superset
SUPERSET_DB_USER=superset
SUPERSET_DB_PASSWORD=superset123
SUPERSET_ADMIN_USERNAME=admin
SUPERSET_ADMIN_EMAIL=admin@datalyptica.local
SUPERSET_ADMIN_PASSWORD=admin123
SUPERSET_SECRET_KEY=<generate>

# Airflow
AIRFLOW_WEBSERVER_PORT=8090
AIRFLOW_DB_NAME=airflow
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow123
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_EMAIL=admin@datalyptica.local
AIRFLOW_ADMIN_PASSWORD=admin123
AIRFLOW_SECRET_KEY=<generate>
AIRFLOW_FERNET_KEY=<generate>
```

### Generating Secret Keys

```bash
# Superset secret key
python -c "import secrets; print(secrets.token_urlsafe(32))"

# Airflow secret key
python -c "import secrets; print(secrets.token_urlsafe(32))"

# Airflow Fernet key
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# JupyterHub proxy token
python -c "import secrets; print(secrets.token_hex(32))"
```

---

## Database Setup

All services require dedicated PostgreSQL databases. These are created automatically on first start, but you can create them manually:

```sql
-- JupyterHub
CREATE DATABASE jupyterhub;
CREATE USER jupyterhub WITH PASSWORD 'jupyterhub123';
GRANT ALL PRIVILEGES ON DATABASE jupyterhub TO jupyterhub;

-- MLflow
CREATE DATABASE mlflow;
CREATE USER mlflow WITH PASSWORD 'mlflow123';
GRANT ALL PRIVILEGES ON DATABASE mlflow TO mlflow;

-- Superset
CREATE DATABASE superset;
CREATE USER superset WITH PASSWORD 'superset123';
GRANT ALL PRIVILEGES ON DATABASE superset TO superset;

-- Airflow
CREATE DATABASE airflow;
CREATE USER airflow WITH PASSWORD 'airflow123';
GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;
```

---

## Quick Start

### 1. Generate Secret Keys

Update `docker/.env` with generated secret keys:

```bash
# Generate all secrets
./scripts/generate-analytics-secrets.sh
```

### 2. Build Images

```bash
cd /path/to/datalyptica
./scripts/build/build-analytics-ml-services.sh
```

### 3. Start Services

```bash
cd docker

# Start Redis first (required by Superset and Airflow)
docker compose up -d redis

# Start all analytics services
docker compose up -d jupyterhub mlflow superset airflow-webserver airflow-scheduler airflow-worker
```

### 4. Verify Services

```bash
# Check status
docker compose ps | grep -E "redis|jupyterhub|mlflow|superset|airflow"

# Check logs
docker logs docker-jupyterhub
docker logs docker-mlflow
docker logs docker-superset
docker logs docker-airflow-webserver
```

### 5. Access Services

- **JupyterHub**: http://localhost:8000 (create account or use admin)
- **MLflow**: http://localhost:5000
- **Superset**: http://localhost:8088 (login with admin credentials)
- **Airflow**: http://localhost:8090 (login with admin credentials)

---

## Usage Examples

### JupyterHub: Data Exploration

```python
# In spawned notebook

# Query Trino/Iceberg
from trino.dbapi import connect
conn = connect(host='trino', port=8080, user='trino')
cursor = conn.cursor()
cursor.execute("SELECT * FROM iceberg.sales.transactions LIMIT 10")
df = cursor.fetchall()

# Track with MLflow
import mlflow
mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("data-exploration")

with mlflow.start_run():
    mlflow.log_param("query", "sales_analysis")
    mlflow.log_metric("row_count", len(df))
```

### MLflow: Model Training

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("sales_prediction")

with mlflow.start_run():
    # Train model
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)

    # Log parameters
    mlflow.log_param("n_estimators", 100)

    # Log metrics
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    mlflow.log_metric("accuracy", accuracy)

    # Log model
    mlflow.sklearn.log_model(model, "model")
```

### Superset: Create Dashboard

1. Go to http://localhost:8088
2. Login with admin credentials
3. **Data** â†’ **Databases** â†’ **+ Database**
4. Select "Trino" and enter connection:
   ```
   trino://trino@trino:8080/iceberg
   ```
5. **Data** â†’ **Datasets** â†’ Import from database
6. **Charts** â†’ **+ Chart** â†’ Select dataset and visualization
7. **Dashboards** â†’ **+ Dashboard** â†’ Add charts

### Airflow: ETL Pipeline

Create DAG in `docker/volumes/airflow_dags/iceberg_etl.py`:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.trino.operators.trino import TrinoOperator
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator

default_args = {
    'owner': 'datalyptica',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'iceberg_etl_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
) as dag:

    # List new files in S3
    list_files = S3ListOperator(
        task_id='list_new_files',
        bucket='lakehouse',
        prefix='raw/',
        aws_conn_id='minio_s3',
    )

    # Transform and load to Iceberg
    transform_load = TrinoOperator(
        task_id='transform_and_load',
        trino_conn_id='trino_iceberg',
        sql="""
        INSERT INTO iceberg.analytics.daily_sales
        SELECT
            date_trunc('day', transaction_date) as date,
            SUM(amount) as total_sales,
            COUNT(*) as transaction_count
        FROM iceberg.raw.transactions
        WHERE transaction_date >= current_date - interval '1' day
        GROUP BY 1
        """
    )

    list_files >> transform_load
```

---

## Integration with Existing Platform

### Data Flow

```
1. Ingestion: Airbyte/Debezium â†’ Kafka â†’ Flink/Spark â†’ Iceberg
                                                            â†“
2. Orchestration: Airflow schedules and monitors pipelines
                                                            â†“
3. Quality: Great Expectations validates data
                                                            â†“
4. Exploration: JupyterHub notebooks query via Trino
                                                            â†“
5. ML Training: MLflow tracks experiments and models
                                                            â†“
6. Visualization: Superset dashboards for stakeholders
```

### Network Architecture

All services are connected to multiple networks for segmentation:

- **data_network**: Processing and compute (Trino, Spark, Flink)
- **storage_network**: Data persistence (PostgreSQL, MinIO, Nessie)
- **management_network**: Monitoring (Prometheus, Grafana)
- **control_network**: Orchestration (Airflow, Kafka)

### Resource Allocation

| Service           | CPUs | Memory | Purpose                  |
| ----------------- | ---- | ------ | ------------------------ |
| Redis             | 1    | 2GB    | Lightweight caching      |
| JupyterHub        | 2    | 4GB    | Multi-user management    |
| MLflow            | 2    | 4GB    | Experiment tracking      |
| Superset          | 4    | 8GB    | BI platform with caching |
| Airflow Webserver | 2    | 4GB    | Web UI                   |
| Airflow Scheduler | 2    | 4GB    | DAG scheduling           |
| Airflow Worker    | 4    | 8GB    | Task execution           |

Total: **17 CPUs, 34GB RAM** for analytics layer

---

## Monitoring

All services expose metrics for Prometheus:

```yaml
# Add to prometheus.yml
scrape_configs:
  - job_name: "jupyterhub"
    static_configs:
      - targets: ["jupyterhub:8081"]

  - job_name: "mlflow"
    static_configs:
      - targets: ["mlflow:5000"]

  - job_name: "superset"
    static_configs:
      - targets: ["superset:8088"]

  - job_name: "airflow"
    static_configs:
      - targets: ["airflow-webserver:8080"]
```

### Key Metrics

- **JupyterHub**: Active users, spawned notebooks, spawn failures
- **MLflow**: Experiments, runs, models registered
- **Superset**: Active dashboards, query performance, cache hit rate
- **Airflow**: DAG runs, task success/failure, queue size

---

## Troubleshooting

### JupyterHub

**Issue**: Cannot spawn notebook  
**Solution**: Check Docker socket permissions, verify spawner image exists

```bash
docker pull ghcr.io/datalyptica/datalyptica/jupyterlab-notebook:v1.0.0
```

**Issue**: Users cannot login  
**Solution**: Check PostgreSQL connection, verify admin user created

### MLflow

**Issue**: Cannot log artifacts  
**Solution**: Verify MinIO bucket exists and credentials are correct

```bash
# Create bucket
docker exec docker-minio mc mb minio/mlflow-artifacts
```

### Superset

**Issue**: Slow dashboard loading  
**Solution**: Check Redis cache, increase worker count

```bash
# Check Redis
docker exec docker-redis redis-cli ping
```

**Issue**: Cannot connect to Trino  
**Solution**: Verify Trino connection string and test manually

```bash
docker exec docker-superset superset test-db trino://trino@trino:8080
```

### Airflow

**Issue**: DAGs not appearing  
**Solution**: Check DAG folder mount, verify Python syntax

```bash
docker exec docker-airflow-webserver airflow dags list
```

**Issue**: Tasks stuck in queue  
**Solution**: Check Celery worker, verify Redis connection

```bash
docker logs docker-airflow-worker
```

---

## Security Considerations

### Authentication

- **JupyterHub**: NativeAuthenticator (DB-based), supports LDAP/OAuth via plugins
- **Superset**: Flask-AppBuilder, integrates with Keycloak via OAuth
- **Airflow**: Basic Auth, can be configured for LDAP/OAuth
- **MLflow**: No built-in auth (use reverse proxy with auth)

### Network Isolation

All services are isolated from external networks. Access only via:

- Internal service-to-service communication
- Exposed ports on localhost (behind firewall)
- Keycloak for SSO (when configured)

### Data Encryption

- All database connections use SSL/TLS (when configured)
- S3/MinIO communication via HTTPS (when enabled)
- Redis connections (password-protected)

---

## Next Steps

1. **Generate Secret Keys**: Update `.env` with secure random keys
2. **Build Images**: Run `./scripts/build/build-analytics-ml-services.sh`
3. **Start Services**: `docker compose up -d redis jupyterhub mlflow superset airflow-webserver`
4. **Configure Connections**: Set up Trino, ClickHouse in Superset and Airflow
5. **Create Sample DAGs**: Add ETL pipelines to `airflow_dags/`
6. **Build Dashboards**: Create visualizations in Superset
7. **Train Models**: Track experiments with MLflow from JupyterHub
8. **Monitor**: Add Prometheus scrape configs for metrics

---

## Documentation References

- **JupyterHub**: https://jupyterhub.readthedocs.io
- **MLflow**: https://mlflow.org/docs/latest/index.html
- **Apache Superset**: https://superset.apache.org/docs/intro
- **Apache Airflow**: https://airflow.apache.org/docs/

---

**Integration Complete**: December 1, 2024  
**Platform Status**: 90% Production Ready (was 85%)
